{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Równanie Bellmana\n",
    "\n",
    "Na zajęciach wyprowadzone zostało równanie Bellmana:\n",
    "\n",
    "$$\\forall_{s\\in S} v_{\\pi}(s) = \\sum_{a\\in A} \\pi (a|s) \\cdot \\sum_{s'\\in S, r\\in R} p(s',r|s,a)\\cdot (r+\\gamma\\cdot v_{\\pi}(s'))$$\n",
    "\n",
    "gdzie $\\pi (a|s)$ oraz $p(s',r|s,a)$ są znane.\n",
    "\n",
    "Jest to układ $|S|$ równań liniowych z $|S|$ niewiadomymi, który ma jednoznaczne rozwiązanie, gdy $\\gamma\\in [0,1)$.\n",
    "\n",
    "Jedną z numerycznych metod rozwiązywania tego układu jest zaczęcie z jakiegokolwiek wektora dla $v_{\\pi}^{(0)}(s)$, wstawienie go do prawej strony równań i w ten sposób otrzymanie nowego wektora $v_{\\pi}^{(1)}(s)$. Iterowanie tej metody zbiegnie do prawdziwego $v_{\\pi}(s)$.\n",
    "\n",
    "Po zbiegnięciu można wybrać alternatywną politykę $\\pi'$ taką, że jest ona zachłanna względem $v_{\\pi}(s)$. Będzie ona na pewno lepsza, czyli $\\pi' \\ge \\pi$. Jeśli będzie $\\pi' = \\pi$, to znaczy, że algorytm się zakończył i mamy $\\pi = \\pi^*$, czyli znaleźliśmy optymalną politykę dla danego środowiska.\n",
    "\n",
    "Powyższy algorytm nazywa się **Programowanie Dynamiczne**.\n",
    "\n",
    "(c, 5 pkt) Mamy $s \\in \\{1, 2, \\ldots , 98, 99\\}$ PLN. Jeśli będziemy mieli $100$, to agent dostaje +10. W każdym innym przypadku agent dostaje 0. Gra kończy się gdy mamy $0$, albo $100$ PLN. Agent może w jednym kroku postawić na szali dowolną liczbę złotych, ale nie mniej niż 1 i nie więcej niż posiada. Szansa wygranej jest $p$, która jest parametrem. Zbadaj $p=0.4$ (strona 84 w książce) oraz $p=0.9$.\n",
    "\n",
    "Jaka jest optymalna strategia grania? Zwizualizuj, rysując wykres słupkowy, gdzie na osi OX jest liczba posiadanych pieniędzy, a na osi OY wartość, którą agent powinien postawić. Czy wynik zależy od $\\gamma$? Jeśli tak / nie, to dlaczego? Zinterpretuj.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cbc9375462760bc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T20:46:39.916823Z",
     "start_time": "2024-03-11T20:46:39.389485Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stany:\n",
    "- $s=0$ - przegrana\n",
    "- $s=100$ - wygrana\n",
    "- $s\\in \\{1, 2, \\ldots , 98, 99\\}$ - stan pośredni\n",
    "- $s\\in \\{0, 1, \\ldots , 98, 99, 100\\}$ - wszystkie stany\n",
    "\n",
    "Akcje:\n",
    "- $a\\in \\{1, 2, \\ldots , min(s, 50, 100-s)\\}$ - stawka, bo więcej niż 50 nie ma sensu stawiać\n",
    "\n",
    "Nagrody:\n",
    "- $r=0$ - $s \\in \\{0, 1, \\ldots , 98, 99\\}$ \n",
    "- $r=10$ - $s=100$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1b6aed5708be73b"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0, policy: 0, V_s: 0\n",
      "state: 1, policy: 1, V_s: 3.1600421742723546\n",
      "state: 2, policy: 1, V_s: 3.901286634904141\n",
      "state: 3, policy: 2, V_s: 4.465287455826703\n",
      "state: 4, policy: 4, V_s: 4.755430416332468\n",
      "state: 5, policy: 4, V_s: 5.161584765607643\n",
      "state: 6, policy: 5, V_s: 5.340916870054279\n",
      "state: 7, policy: 7, V_s: 5.4711539116386625\n",
      "state: 8, policy: 7, V_s: 5.870901748558602\n",
      "state: 9, policy: 7, V_s: 6.021211073979172\n",
      "state: 10, policy: 8, V_s: 6.1410885860237725\n",
      "state: 11, policy: 8, V_s: 6.242608733789835\n",
      "state: 12, policy: 9, V_s: 6.310272204412439\n",
      "state: 13, policy: 10, V_s: 6.374070320591957\n",
      "state: 14, policy: 13, V_s: 6.75451100202304\n",
      "state: 15, policy: 13, V_s: 6.8969110529309745\n",
      "state: 16, policy: 13, V_s: 7.000117625725679\n",
      "state: 17, policy: 14, V_s: 7.081902520595659\n",
      "state: 18, policy: 15, V_s: 7.148114554175803\n",
      "state: 19, policy: 14, V_s: 7.2107813120560875\n",
      "state: 20, policy: 14, V_s: 7.2466178219074955\n",
      "state: 21, policy: 13, V_s: 7.294316460972884\n",
      "state: 22, policy: 14, V_s: 7.339034955099231\n",
      "state: 23, policy: 17, V_s: 7.37307956736735\n",
      "state: 24, policy: 16, V_s: 7.420778206432739\n",
      "state: 25, policy: 16, V_s: 7.447413050803626\n",
      "state: 26, policy: 25, V_s: 7.858807591369025\n",
      "state: 27, policy: 25, V_s: 7.98778667449201\n",
      "state: 28, policy: 25, V_s: 8.08122871085136\n",
      "state: 29, policy: 25, V_s: 8.145977474939846\n",
      "state: 30, policy: 25, V_s: 8.20222798682582\n",
      "state: 31, policy: 26, V_s: 8.246946480952168\n",
      "state: 32, policy: 26, V_s: 8.280991093220289\n",
      "state: 33, policy: 25, V_s: 8.328689732285678\n",
      "state: 34, policy: 24, V_s: 8.353006547657543\n",
      "state: 35, policy: 24, V_s: 8.3729323370405\n",
      "state: 36, policy: 22, V_s: 8.408214565097477\n",
      "state: 37, policy: 22, V_s: 8.431819545763204\n",
      "state: 38, policy: 27, V_s: 8.449777206280688\n",
      "state: 39, policy: 25, V_s: 8.487027563203139\n",
      "state: 40, policy: 24, V_s: 8.509132159336376\n",
      "state: 41, policy: 24, V_s: 8.525313647093212\n",
      "state: 42, policy: 24, V_s: 8.53701456057158\n",
      "state: 43, policy: 24, V_s: 8.547904495409282\n",
      "state: 44, policy: 23, V_s: 8.555422658811795\n",
      "state: 45, policy: 19, V_s: 8.586414256244279\n",
      "state: 46, policy: 30, V_s: 8.607310965276868\n",
      "state: 47, policy: 30, V_s: 8.621990213225901\n",
      "state: 48, policy: 29, V_s: 8.63358930445734\n",
      "state: 49, policy: 18, V_s: 8.64115936060993\n",
      "state: 50, policy: 50, V_s: 9.0\n",
      "state: 51, policy: 49, V_s: 9.351115797141373\n",
      "state: 52, policy: 48, V_s: 9.427988737469922\n",
      "state: 53, policy: 47, V_s: 9.480682518304885\n",
      "state: 54, policy: 46, V_s: 9.528381157370275\n",
      "state: 55, policy: 45, V_s: 9.55269797274214\n",
      "state: 56, policy: 44, V_s: 9.567924498397119\n",
      "state: 57, policy: 43, V_s: 9.607905990182074\n",
      "state: 58, policy: 42, V_s: 9.630010586315311\n",
      "state: 59, policy: 41, V_s: 9.643330309875822\n",
      "state: 60, policy: 40, V_s: 9.652195603971675\n",
      "state: 61, policy: 39, V_s: 9.66051314595893\n",
      "state: 62, policy: 38, V_s: 9.667870038578947\n",
      "state: 63, policy: 37, V_s: 9.707292683223212\n",
      "state: 64, policy: 36, V_s: 9.727310583976623\n",
      "state: 65, policy: 35, V_s: 9.738200518814324\n",
      "state: 66, policy: 34, V_s: 9.745289198389825\n",
      "state: 67, policy: 33, V_s: 9.751770589289178\n",
      "state: 68, policy: 32, V_s: 9.756739310858773\n",
      "state: 69, policy: 31, V_s: 9.760479948565262\n",
      "state: 70, policy: 30, V_s: 9.765821894340274\n",
      "state: 71, policy: 29, V_s: 9.768331310451442\n",
      "state: 72, policy: 28, V_s: 9.769988039293061\n",
      "state: 73, policy: 27, V_s: 9.774657986874917\n",
      "state: 74, policy: 26, V_s: 9.777023037401161\n",
      "state: 75, policy: 25, V_s: 9.81\n",
      "state: 76, policy: 24, V_s: 9.848518986372293\n",
      "state: 77, policy: 23, V_s: 9.857554304163324\n",
      "state: 78, policy: 22, V_s: 9.86111320485574\n",
      "state: 79, policy: 21, V_s: 9.866700952768378\n",
      "state: 80, policy: 20, V_s: 9.86869760435745\n",
      "state: 81, policy: 19, V_s: 9.870108303472104\n",
      "state: 82, policy: 18, V_s: 9.875457952557896\n",
      "state: 83, policy: 17, V_s: 9.877076027855084\n",
      "state: 84, policy: 16, V_s: 9.87810653797729\n",
      "state: 85, policy: 15, V_s: 9.878923970490625\n",
      "state: 86, policy: 14, V_s: 9.879298923536375\n",
      "state: 87, policy: 13, V_s: 9.879932073366104\n",
      "state: 88, policy: 12, V_s: 9.886366708773506\n",
      "state: 89, policy: 11, V_s: 9.887500188437016\n",
      "state: 90, policy: 10, V_s: 9.888182784392171\n",
      "state: 91, policy: 9, V_s: 9.88879121573021\n",
      "state: 92, policy: 8, V_s: 9.889029588417955\n",
      "state: 93, policy: 7, V_s: 9.889136903118274\n",
      "state: 94, policy: 6, V_s: 9.889773003789616\n",
      "state: 95, policy: 5, V_s: 9.889936450595295\n",
      "state: 96, policy: 4, V_s: 9.890012662957616\n",
      "state: 97, policy: 3, V_s: 9.890079570341065\n",
      "state: 98, policy: 2, V_s: 9.890101139666186\n",
      "state: 99, policy: 1, V_s: 9.890109102569957\n",
      "state: 100, policy: 0, V_s: 0\n"
     ]
    }
   ],
   "source": [
    "gamma=0.9\n",
    "p=0.9\n",
    "# numbers from 0 to 100 do it without numpy\n",
    "states = list(range(101))\n",
    "win_state=100; lose_state=0;\n",
    "\n",
    "def available_actions(s): # ograniczam trochę polityki, bo inne akcje są bez sensu\n",
    "    return np.arange(1, min(s,100-s, 50)+1)\n",
    "\n",
    "def reward(next_state): # nagroda powinna być na podstawie następnego statu\n",
    "    if next_state == 100:\n",
    "        return 10\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def available_next_states(s, a):\n",
    "    return [s+a, s-a]\n",
    "\n",
    "def transition_probability(s, a, next_s):\n",
    "    if next_s == s+a:\n",
    "        return p\n",
    "    elif next_s == s-a:\n",
    "        return 1-p\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "V_s = [0]*101\n",
    "policy = [0]*101\n",
    "\n",
    "def update_v_bellman(s, policy): #???\n",
    "    best_action = policy[s]\n",
    "    best_action_value = float(\"-inf\")\n",
    "    for a in available_actions(s):\n",
    "        action_value = 0\n",
    "        for next_s in available_next_states(s, a):\n",
    "            action_value += transition_probability(s, a, next_s) * (reward(next_s) + gamma * V_s[next_s])\n",
    "        if action_value > best_action_value:\n",
    "            best_action_value = action_value\n",
    "            best_action = a\n",
    "    V_s[s] = best_action_value\n",
    "    policy[s] = best_action\n",
    "    return V_s, policy\n",
    "\n",
    "for _ in range(200):\n",
    "    # without 0 and 100\n",
    "    for s in states[1:-1]:\n",
    "        V_s, policy = update_v_bellman(s, policy)\n",
    "        \n",
    "# print policy and V_s with numbers\n",
    "for s in states:\n",
    "    print(f\"state: {s}, policy: {policy[s]}, V_s: {V_s[s]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T18:09:14.684009Z",
     "start_time": "2024-03-12T18:09:14.369527Z"
    }
   },
   "id": "ac6caf935665ad2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bellman(p, gamma, n=100):\n",
    "    v = np.zeros(n+1)\n",
    "    for s in range(1, n):\n",
    "        v[s] = max([p * (v[s+a] if s+a < n else 10) + (1-p) * (v[s-a] if s-a > 0 else 0) for a in range(1, s+1)])\n",
    "    return v"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "518699b247133104"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
